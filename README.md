# A Survey on Spatio-Temporal Prediction: from Transformers to Foundation Models

## Survey Paper

[**A Survey on Spatio-Temporal Prediction: from Transformers to Foundation Models**]() ()

Yingchi Mao, Hongliang Zhou, Rongzhi Qi, Ling Chen, Zhende Sun, Yi Rong, Xiaoming He, Mingkai Chen, Shahid Mumtaz, Valerio Frascolla, Mohsen Guizani and Joel J. P. C. Rodrigues

If you find this repository helpful for your work, please kindly cite our survey paper.

```bibtex
@inproceedings{
}
```

## EXPLORING TRANSFORMERS: TECHNIQUES AND METHODS

### Module Enhancement

+ Group normalization, in *ECCV* 2018. [[paper]](https://openaccess.thecvf.com/content_ECCV_2018/html/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.html) [[official code]](https://github.com/pytorch/pytorch/blob/1c5c289b6218eb1026dcb5fd9738231401cfccea/torch/nn/modules/normalization.py#L177)

+ Maximum Norm Minimization: A Single-Policy Multi-Objective Reinforcement Learning to Expansion of the Pareto Front, in *CIKM* 2022. [[paper]](https://dl.acm.org/doi/abs/10.1145/3511808.3557389)

+ Deep Multi-Representation Model for Click-Through Rate Prediction, in *IJCNN* 2023. [[paper]](https://ieeexplore.ieee.org/abstract/document/10191755/) [[official code]](https://github.com/Shereen-Elsayed/DeepMR)

### Architecture Adjustment

+ Informer: Beyond efficient transformer for long sequence time-series forecasting, in *AAAI* 2021. [[paper]](http://ojs.aaai.org/index.php/AAAI/article/view/17325) [[official code]](https://github.com/zhouhaoyi/Informer2020)

+ DeepNet: Scaling transformers to 1,000 layers, in *IEEE Transactions on Pattern Analysis and Machine Intelligence* 2024. [[paper]](https://dl.acm.org/doi/abs/10.1109/TPAMI.2024.3386927) [[official code]](https://github.com/microsoft/unilm)

+ Crossgnn: Confronting noisy multivariate time series via cross interaction refinement, in *NeurIPS* 2023. [[paper]](https://proceedings.neurips.cc/paper_files/paper/2023/hash/9278abf072b58caf21d48dd670b4c721-Abstract-Conference.html) [[official code]](https://github.com/hqh0728/crossgnn)

+ Graph transformer networks, in *NeurIPS* 2019. [[paper]](https://dl.acm.org/doi/10.5555/3454287.3455360) [[official code]](https://github.com/seongjunyun/Graph_Transformer_Networks)

+ Graphformers: Gnn-nested transformers for representation learning on textual graph, in *NeurIPS* 2021. [[paper]](https://proceedings.neurips.cc/paper_files/paper/2021/hash/f18a6d1cde4b205199de8729a6637b42-Abstract.html) [[official code]](https://github.com/microsoft/graphformers)

### Foundation Models

+ Time-llm: Time series forecasting by reprogramming large language models, in *arXiv* 2020.[[paper]](https://arxiv.org/abs/2310.01728) [[official code]](https://github.com/kimmeen/time-llm)