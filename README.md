# A Survey on Spatio-Temporal Prediction: from Transformers to Foundation Models

## Survey Paper

[**A Survey on Spatio-Temporal Prediction: from Transformers to Foundation Models**]() ()

Yingchi Mao, Hongliang Zhou, Rongzhi Qi, Ling Chen, Zhende Sun, Yi Rong, Xiaoming He, Mingkai Chen, Shahid Mumtaz, Valerio Frascolla, Mohsen Guizani and Joel J. P. C. Rodrigues

If you find this repository helpful for your work, please kindly cite our survey paper.

```bibtex
@inproceedings{
}
```

## EXPLORING TRANSFORMERS: TECHNIQUES AND METHODS

### Module Enhancement

+ Efficient transformers: A survey, in *Comput. Surveys* 2022. [[paper]](https://dl.acm.org/doi/full/10.1145/3530811)

+ Skeleton-based action recognition with channel enhanced local graph window transformer, in *ICACIC* 2023. [[paper]](https://dl.acm.org/doi/abs/10.1145/3594315.3594643)

+ Synthesizer: Rethinking self-attention for transformer models, in *ICML* 2021. [[paper]](https://proceedings.mlr.press/v139/tay21a.html)

+ Positional Encoding-based Resident Identification in Multi-resident Smart Homes, in *ACM Transactions on Internet Technology* 2023. [[paper]](https://dl.acm.org/doi/abs/10.1145/3631353)

+ Airformer: Predicting nationwide air quality in china with transformers, in *AAAI* 2023. [[paper]](https://ojs.aaai.org/index.php/AAAI/article/view/26676) [[official code]](https://github.com/yoshall/airformer)

+ Deep sparse rectifier neural networks, in *AISTATS* 2011. [[paper]](https://proceedings.mlr.press/v15/glorot11a)

+ DPReLU: Dynamic Parametric Rectified Linear Unit, in *SMA* 2021. [[paper]](https://dl.acm.org/doi/abs/10.1145/3426020.3426049)

+ An efficient spatial-temporal model based on gated linear units for trajectory prediction, in *Neurocomputing* 2022. [[paper]](https://doi.org/10.1016/j.neucom.2021.12.051)

+ On layer normalization in the transformer architecture, in *ICML* 2020. [[paper]](https://proceedings.mlr.press/v119/xiong20b)

+ Group normalization, in *ECCV* 2018. [[paper]](https://openaccess.thecvf.com/content_ECCV_2018/html/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.html) [[official code]](https://github.com/pytorch/pytorch/blob/1c5c289b6218eb1026dcb5fd9738231401cfccea/torch/nn/modules/normalization.py#L177)

+ Maximum Norm Minimization: A Single-Policy Multi-Objective Reinforcement Learning to Expansion of the Pareto Front, in *CIKM* 2022. [[paper]](https://dl.acm.org/doi/abs/10.1145/3511808.3557389)

+ Deep Multi-Representation Model for Click-Through Rate Prediction, in *IJCNN* 2023. [[paper]](https://ieeexplore.ieee.org/abstract/document/10191755/) [[official code]](https://github.com/Shereen-Elsayed/DeepMR)

### Architecture Adjustment

+ Informer: Beyond efficient transformer for long sequence time-series forecasting, in *AAAI* 2021. [[paper]](http://ojs.aaai.org/index.php/AAAI/article/view/17325) [[official code]](https://github.com/zhouhaoyi/Informer2020)

+ DeepNet: Scaling transformers to 1,000 layers, in *IEEE Transactions on Pattern Analysis and Machine Intelligence* 2024. [[paper]](https://dl.acm.org/doi/abs/10.1109/TPAMI.2024.3386927) [[official code]](https://github.com/microsoft/unilm)

+ Crossgnn: Confronting noisy multivariate time series via cross interaction refinement, in *NeurIPS* 2023. [[paper]](https://proceedings.neurips.cc/paper_files/paper/2023/hash/9278abf072b58caf21d48dd670b4c721-Abstract-Conference.html) [[official code]](https://github.com/hqh0728/crossgnn)

+ Graph transformer networks, in *NeurIPS* 2019. [[paper]](https://dl.acm.org/doi/10.5555/3454287.3455360) [[official code]](https://github.com/seongjunyun/Graph_Transformer_Networks)

+ Graphformers: Gnn-nested transformers for representation learning on textual graph, in *NeurIPS* 2021. [[paper]](https://proceedings.neurips.cc/paper_files/paper/2021/hash/f18a6d1cde4b205199de8729a6637b42-Abstract.html) [[official code]](https://github.com/microsoft/graphformers)

### Foundation Models

+ Time-llm: Time series forecasting by reprogramming large language models, in *arXiv* 2020.[[paper]](https://arxiv.org/abs/2310.01728) [[official code]](https://github.com/kimmeen/time-llm)